# -*- coding: utf-8 -*-
"""project1_AI_Helper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-b6P-s4i3B_duN5VOh0dnb_elFlx9N8
"""

!git clone https://github.com/spMohanty/PlantVillage-Dataset.git

import os
import shutil

source_path = "/content/PlantVillage-Dataset/raw/color"
target_path = "/content/tomato_only"

os.makedirs(target_path, exist_ok=True)

for folder in os.listdir(source_path):
    if folder.startswith("Tomato___"):
        shutil.copytree(os.path.join(source_path, folder),
                        os.path.join(target_path, folder))

print("âœ… Only Tomato classes copied successfully!")

import os
import random
import shutil

source = "/content/PlantVillage-Dataset/raw/color"
target = "/content/tomato_small"

os.makedirs(target, exist_ok=True)

for folder in os.listdir(source):
    if folder.startswith("Tomato___"):
        full_folder_path = os.path.join(source, folder)
        selected_images = random.sample(os.listdir(full_folder_path), 300)

        new_folder = os.path.join(target, folder)
        os.makedirs(new_folder, exist_ok=True)

        for img in selected_images:
            shutil.copy(os.path.join(full_folder_path, img), os.path.join(new_folder, img))

print("âœ… Created small tomato dataset (300 images per class)")

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam

# Use only tomato data
data_path = "/content/tomato_small"

# Preprocessing
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_data = datagen.flow_from_directory(
    data_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_data = datagen.flow_from_directory(
    data_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# Build CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.3),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(train_data.num_classes, activation='softmax')
])

model.compile(optimizer=Adam(0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_data, epochs=10, validation_data=val_data)

model.save("tomato_leaf_model.h5")
print("âœ… Model saved successfully!")

from tensorflow.keras.models import load_model
import numpy as np
from tensorflow.keras.preprocessing import image

# Load the trained model
model = load_model("tomato_leaf_model.h5")

# Get class names from the data generator
class_names = list(train_data.class_indices.keys())
print("âœ… Model Loaded. Classes:", class_names)

import os
import random
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt

# Load trained model (only if not loaded already)
from tensorflow.keras.models import load_model
model = load_model("tomato_leaf_model.h5")

# Get class names (ensure this matches training order)
class_names = list(train_data.class_indices.keys())

# Path to your dataset folders
test_base = "/content/tomato_only"  # Or 'tomato_small' if you used fewer images

# Function to predict one image
def predict_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    prediction = model.predict(img_array)
    class_index = np.argmax(prediction)
    predicted_class = class_names[class_index]
    confidence = prediction[0][class_index]

    return predicted_class, confidence, img

# Predict 5 images from each folder
for category in os.listdir(test_base):
    category_path = os.path.join(test_base, category)

    if not os.path.isdir(category_path):
        continue  # Skip if it's not a folder

    image_files = [f for f in os.listdir(category_path) if f.lower().endswith(('jpg', 'jpeg', 'png'))]

    if len(image_files) < 5:
        continue  # Skip small folders

    sample_images = random.sample(image_files, 5)

    print(f"\nðŸ” Actual Category: {category}")
    plt.figure(figsize=(15, 5))

    for i, image_name in enumerate(sample_images):
        img_path = os.path.join(category_path, image_name)
        predicted_class, confidence, img = predict_image(img_path)

        plt.subplot(1, 5, i + 1)
        plt.imshow(img)
        plt.title(f"Pred: {predicted_class.split('___')[-1]}\nConf: {confidence*100:.1f}%")
        plt.axis('off')

    plt.suptitle(f"Actual Category: {category}", fontsize=16)
    plt.tight_layout()
    plt.show()

from google.colab import files
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model

# âœ… Load your trained model
model = load_model("tomato_leaf_model.h5")

# âœ… Set class names from training data
class_names = sorted(os.listdir("/content/tomato_only"))  # or "tomato_small"

# âœ… Upload your own leaf image
uploaded = files.upload()  # Select one or more image files

# âœ… Predict each uploaded image
for filename in uploaded.keys():
    img = image.load_img(filename, target_size=(224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    prediction = model.predict(img_array)
    class_index = np.argmax(prediction)
    predicted_class = class_names[class_index]
    confidence = prediction[0][class_index]

    # âœ… Show image and prediction
    plt.imshow(img)
    plt.title(f"Prediction: {predicted_class.split('___')[-1]}\nConfidence: {confidence*100:.2f}%")
    plt.axis('off')
    plt.show()

from google.colab import files
uploaded = files.upload()

import zipfile

with zipfile.ZipFile("Recording_voice_files.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/")

print("âœ… Voice files unzipped!")

pip install SpeechRecognition

!unzip -l Recording_voice_files.zip

!pip install pydub
!apt-get install ffmpeg

import os
import speech_recognition as sr
from pydub import AudioSegment

import zipfile

with zipfile.ZipFile("Recording_voice_files.zip", 'r') as zip_ref:
    zip_ref.extractall("Recording_voice_files")  # Extract into folder



from pydub import AudioSegment
import os

input_folder = "/content/Recording_voice_files/Recording_voice_files/telugu_records"
output_folder = "/content/Recording_voice_files/Recording_voice_files/telugu_records_wav"
os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(input_folder):
    if filename.endswith(".m4a"):
        input_path = os.path.join(input_folder, filename)
        output_path = os.path.join(output_folder, filename.replace(".m4a", ".wav"))

        audio = AudioSegment.from_file(input_path)
        audio.export(output_path, format="wav")
        print(f"âœ… Converted: {filename} â†’ {output_path}")

import os
import speech_recognition as sr

recognizer = sr.Recognizer()

# âœ… Correct file path based on your structure
voice_path = "/content/Recording_voice_files/Recording_voice_files/telugu_records_wav/8_Tomato_Yellow_Leaf_Curl_Virus.wav"

def get_telugu_solution(text):
    if "à°¬à±à°²à°¾à°•à±" in text or "à°¸à±à°ªà°¾à°Ÿà±" in text or "à°°à°¿à°‚à°—à±" in text or "à°šà±€à°•à°Ÿà°¿" in text:
        return "âš« à°†à°•à±à°²à°ªà±ˆ à°¨à°²à±à°ªà± à°²à±‡à°¦à°¾ à°°à°¿à°‚à°—à± à°®à°šà±à°šà°²à±: à°¶à°¿à°²à±€à°‚à°§à±à°°à°‚ à°²à±‡à°¦à°¾ à°¬à±à°¯à°¾à°•à±à°Ÿà±€à°°à°¿à°¯à°¾ à°¸à°®à°¸à±à°¯.\nðŸ’Š à°•à°¾à°ªà°°à± à°«à°‚à°—à°¿à°¸à±ˆà°¡à± à°µà°¾à°¡à°‚à°¡à°¿."
    elif "à°ªà°šà±à°š" in text or "à°Žà°²à±à°²à±‹" in text:
        return "ðŸŸ¡ à°†à°•à±à°²à± à°ªà°šà±à°šà°—à°¾ à°®à°¾à°°à°¡à°‚: à°ªà±‹à°·à°• à°²à±‹à°ªà°‚ à°²à±‡à°¦à°¾ à°µà±ˆà°°à°¸à±.\nðŸ’Š NPK à°Žà°°à±à°µà±à°²à± à°µà±‡à°¸à°¿ à°¤à±†à°²à±à°²à°¦à±‹à°®à°²à± à°¨à°¿à°¯à°‚à°¤à±à°°à°¿à°‚à°šà°‚à°¡à°¿."
    elif "à°®à±‹à°²à±à°¡à±" in text or "à°¤à±†à°²à±à°²à°—à°¾" in text or "à°ªà±Šà°¡à°¿" in text:
        return "âšª à°†à°•à±à°² à°•à°¿à°‚à°¦ à°¤à±†à°²à±à°²à°Ÿà°¿ à°²à±‡à°¦à°¾ à°ªà°šà±à°šà°Ÿà°¿ à°®à°šà±à°šà°²à±: à°‡à°¦à°¿ à°²à±€à°«à± à°®à±‹à°²à±à°¡à± à°•à°¾à°µà°šà±à°šà±.\nðŸ’Š à°¨à°¿à°®à± à°†à°¯à°¿à°²à± à°²à±‡à°¦à°¾ à°¸à°²à±à°«à°°à± à°¸à±à°ªà±à°°à±‡ à°µà°¾à°¡à°‚à°¡à°¿."
    elif "à°¨à±†à°Ÿà±" in text or "à°œà°¾à°²à°‚" in text or "à°¸à±à°ªà±ˆà°¡à°°à±" in text:
        return "ðŸ•¸ï¸ à°¸à±à°ªà±ˆà°¡à°°à± à°®à±ˆà°Ÿà±à°¸à± à°¸à°®à°¸à±à°¯.\nðŸ’Š à°¨à°¿à°®à± à°†à°¯à°¿à°²à± à°²à±‡à°¦à°¾ à°‡à°¨à±à°¸à±†à°•à±à°Ÿà°¿à°¸à±ˆà°¡à°²à± à°¸à±Šà°ªà± à°µà°¾à°¡à°‚à°¡à°¿."
    elif "à°µà°‚à°ªà±" in text or "à°•à°°à±à°²à±" in text or "à°ªà°—à°²à°¡à°‚" in text:
        return "ðŸŒªï¸ à°†à°•à±à°² à°µà°‚à°ªà± à°²à±‡à°¦à°¾ à°ªà°—à°²à°¡à°‚: à°µà±ˆà°°à°¸à± à°¸à±‹à°•à°¿à°¨ à°®à±Šà°•à±à°•.\nðŸ’Š à°¬à°¾à°§à°¿à°¤ à°®à±Šà°•à±à°•à°²à± à°¤à±Šà°²à°—à°¿à°‚à°šà°‚à°¡à°¿ à°®à°°à°¿à°¯à± à°ªà±à°°à±à°—à±à°®à°‚à°¦à±à°²à± à°ªà±‚à°¯à°‚à°¡à°¿."
    elif "à°¹à±†à°²à±à°¤à±€" in text or "à°¸à°®à°¸à±à°¯ à°²à±‡à°¦à±" in text or "à°ªà°°à±à°µà°¾à°²à±‡à°¦à±" in text:
        return "âœ… à°ˆ à°®à±Šà°•à±à°• à°†à°°à±‹à°—à±à°¯à°‚à°—à°¾ à°‰à°‚à°¦à°¿. à°Žà°²à°¾à°‚à°Ÿà°¿ à°¸à°®à°¸à±à°¯ à°²à±‡à°¦à±!"
    else:
        return "ðŸ¤” à°¸à±à°ªà°·à±à°Ÿà°‚à°—à°¾ à°—à±à°°à±à°¤à°¿à°‚à°šà°²à±‡à°•à°ªà±‹à°¯à°¾à°‚. à°®à°°à°¿à°‚à°¤ à°¸à±à°ªà°·à±à°Ÿà°‚à°—à°¾ à°šà±†à°ªà±à°ªà°‚à°¡à°¿ à°²à±‡à°¦à°¾ à°•à±Šà°¤à±à°¤à°—à°¾ à°°à°¿à°•à°¾à°°à±à°¡à± à°šà±‡à°¯à°‚à°¡à°¿."

# ðŸ”„ Recognize speech from selected file
file_path = voice_path
print(f"\nðŸŽ§ à°«à±ˆà°²à±: {os.path.basename(file_path)}")
with sr.AudioFile(file_path) as source:
    audio_data = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio_data, language='te-IN')
        print(f"ðŸ—£ï¸ à°®à°¾à°Ÿà°²à±: {text}")
        print(get_telugu_solution(text))
    except sr.UnknownValueError:
        print("âŒ à°¸à±à°ªà±€à°šà± à°…à°°à±à°¥à°‚ à°•à°¾à°²à±‡à°¦à±.")
    except sr.RequestError:
        print("âŒ Google API à°²à±‹ à°¸à°®à°¸à±à°¯ à°‰à°‚à°¦à°¿.")

from pydub import AudioSegment
import os

input_folder = "/content/Recording_voice_files/Recording_voice_files/english_records"
output_folder = "/content/Recording_voice_files/Recording_voice_files/english_records_wav"
os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(input_folder):
    if filename.endswith(".mp3"):
        input_path = os.path.join(input_folder, filename)
        output_path = os.path.join(output_folder, filename.replace(".mp3", ".wav"))

        audio = AudioSegment.from_file(input_path)
        audio.export(output_path, format="wav")
        print(f"âœ… Converted: {filename} â†’ {output_path}")

import speech_recognition as sr

recognizer = sr.Recognizer()
voice_path = "/content/Recording_voice_files/Recording_voice_files/english_records_wav"

def get_english_solution(text):
    text = text.lower()
    if "yellow" in text and "leaves" in text:
        return "ðŸŸ¡ Yellowing leaves: Possible nutrient deficiency or viral issue.\nðŸ’Š Apply NPK fertilizer and control whiteflies."
    elif "black spots" in text or "dark" in text:
        return "âš« Black/Dark spots: Possibly early blight or fungal infection.\nðŸ’Š Use copper-based fungicide."
    elif "mold" in text or "white" in text:
        return "âšª White mold or powdery mildew.\nðŸ’Š Use neem oil or sulfur spray."
    elif "web" in text:
        return "ðŸ•¸ï¸ Web-like symptoms: Likely spider mites.\nðŸ’Š Spray neem oil or insecticidal soap."
    elif "curl" in text or "twist" in text:
        return "ðŸŒªï¸ Curling leaves: Could be leaf curl virus.\nðŸ’Š Remove infected leaves and spray appropriate insecticide."
    else:
        return "ðŸ¤” Couldn't detect issue. Please re-record or add an image for better results."

# Loop through converted WAV files
for file in os.listdir(voice_path):
    if file.endswith(".wav"):
        file_path = os.path.join(voice_path, file)
        print(f"\nðŸŽ§ Processing: {file}")
        with sr.AudioFile(file_path) as source:
            audio_data = recognizer.record(source)
            try:
                text = recognizer.recognize_google(audio_data)
                print(f"ðŸ—£ï¸ Spoken Text: {text}")
                print(get_english_solution(text))
            except sr.UnknownValueError:
                print("âŒ Could not understand the speech.")
            except sr.RequestError:
                print("âŒ Google API issue.")

from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import os

# Load the model (adjust path if needed)
model = load_model("tomato_leaf_model.h5")

# Load class names from training data
class_names = list(train_data.class_indices.keys())

def predict_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    prediction = model.predict(img_array)[0]
    class_index = np.argmax(prediction)
    confidence = prediction[class_index]

    return class_names[class_index], confidence

import speech_recognition as sr

def transcribe_voice(wav_path):
    recognizer = sr.Recognizer()
    with sr.AudioFile(wav_path) as source:
        audio = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio)
            return text.lower()
        except sr.UnknownValueError:
            return "âŒ Could not understand the speech."
        except sr.RequestError:
            return "âŒ Google API error."
        except Exception as e:
            return f"âš ï¸ Error: {str(e)}"

# âœ… Example usage:
voice_file_path = "/content/Recording_voice_files/Recording_voice_files/english_records_wav/_Leaves_turning_yell.wav"

# Call the function and print the result
result = transcribe_voice(voice_file_path)
print(f"ðŸ—£ï¸ Transcribed Text:\n{result}")

import speech_recognition as sr

def transcribe_voice(wav_path):
    recognizer = sr.Recognizer()
    with sr.AudioFile(wav_path) as source:
        audio = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio)
            return text.lower()
        except:
            return ""

def smart_solution(image_label, voice_text):
    suggestions = []

    if "yellow" in voice_text or "nutrient" in voice_text:
        suggestions.append("ðŸŸ¡ Use balanced fertilizer (NPK).")

    if "black" in voice_text or "spot" in voice_text or "blight" in image_label:
        suggestions.append("âš« Use copper-based fungicide.")

    if "mold" in voice_text or "white" in voice_text:
        suggestions.append("âšª Apply neem oil or sulfur spray.")

    if "curl" in voice_text:
        suggestions.append("ðŸŒªï¸ Remove curled leaves and control insects.")

    if not suggestions:
        suggestions.append("ðŸ¤” Try to upload a clearer image or re-record voice.")

    return "\n".join(suggestions)
# Suppose these are the model's output and transcribed voice:
image_label = "Tomato___Late_blight"
voice_text = "the leaves are yellow and curling a bit"

# Call the function and print the solution
print("ðŸ’Š Smart Solution:\n")
print(smart_solution(image_label, voice_text))

image_path = "/content/tomato_leaf.jpg"  # ðŸ–¼ï¸ Replace with your test image
voice_path = "/content/Recording_voice_files/Recording_voice_files/english_records_wav/_Leaves_turning_yell.wav"  # ðŸŽ¤

# Predict from image
img_label, img_conf = predict_image(image_path)
print(f"ðŸ“¸ Image Prediction: {img_label} ({img_conf*100:.2f}%)")

# Predict from voice
voice_text = transcribe_voice(voice_path)
print(f"ðŸ—£ï¸ Voice Text: {voice_text}")

# Final suggestion
print("ðŸ’Š Suggested Action:\n", smart_solution(img_label, voice_text))

!pip install flask tensorflow pillow numpy speechrecognition pydub



from google.colab import files
uploaded = files.upload()

from tensorflow.keras.models import load_model
model = load_model('/content/tomato_leaf_model.h5')

!pip install gradio tensorflow keras speechrecognition --quiet

import gradio as gr
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import speech_recognition as sr
from PIL import Image

model = load_model("/content/tomato_leaf_model.h5")  # Change path if needed
class_names = ['Bacterial Spot', 'Early Blight', 'Late Blight', 'Leaf Mold',
               'Septoria Leaf Spot', 'Spider Mites', 'Target Spot',
               'Yellow Leaf Curl Virus', 'Mosaic Virus', 'Healthy']

def predict_image(img):
    img = img.resize((224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    prediction = model.predict(img_array)[0]
    class_index = np.argmax(prediction)
    confidence = round(prediction[class_index] * 100, 2)
    return class_names[class_index], confidence

def transcribe_voice(audio_path):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_path) as source:
        audio = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio, language='te-IN')
            return text
        except:
            return "âŒ Unable to recognize voice."

def smart_solution(image_label, voice_text):
    suggestions = []

    if "yellow" in voice_text or "nutrient" in voice_text:
        suggestions.append("ðŸŸ¡ Use balanced fertilizer (NPK).")

    if "black" in voice_text or "spot" in voice_text or "blight" in image_label.lower():
        suggestions.append("âš« Use copper-based fungicide.")

    if "mold" in voice_text or "white" in voice_text:
        suggestions.append("âšª Apply neem oil or sulfur spray.")

    if "curl" in voice_text or "à°µà°‚à°ªà±" in voice_text:
        suggestions.append("ðŸŒªï¸ Remove curled leaves and control whiteflies.")

    if "healthy" in voice_text or "à°¸à°®à°¸à±à°¯ à°²à±‡à°¦à±" in voice_text:
        suggestions.append("âœ… Plant is healthy. No action needed.")

    if not suggestions:
        suggestions.append("ðŸ¤” Try a clearer image or re-record the voice.")

    return "\n".join(suggestions)

def ai_farmer_helper(img, audio):
    try:
        img_label, conf = predict_image(img)
        voice_text = transcribe_voice(audio)
        suggestion = smart_solution(img_label, voice_text)

        return f"ðŸ“¸ Prediction: {img_label} ({conf}%)", f"ðŸ—£ï¸ Voice: {voice_text}", f"ðŸ’Š Suggestion:\n{suggestion}"
    except Exception as e:
        return "Error", "Error", f"Error: {str(e)}"

!pip install pydub
!apt install ffmpeg

from pydub import AudioSegment
import os

def convert_to_wav(filepath):
    if filepath.endswith(".wav"):
        return filepath  # already in correct format

    sound = AudioSegment.from_file(filepath)
    wav_path = filepath.replace(".webm", ".wav").replace(".mp3", ".wav")
    sound.export(wav_path, format="wav")
    return wav_path

def transcribe_microphone(audio):
    recognizer = sr.Recognizer()

    try:
        wav_file = convert_to_wav(audio)  # Ensure format is supported
        with sr.AudioFile(wav_file) as source:
            data = recognizer.record(source)
        text = recognizer.recognize_google(data, language='te-IN')  # Telugu
        return text
    except Exception as e:
        return f"âŒ Error: {str(e)}"

# âœ… Convert audio to WAV
def convert_to_wav(filepath):
    if filepath.endswith(".wav"):
        return filepath
    sound = AudioSegment.from_file(filepath)
    wav_path = filepath.rsplit(".", 1)[0] + ".wav"
    sound.export(wav_path, format="wav")
    return wav_path

# âœ… Main function (gradio interface will call this)
def process(image_input, audio_input):
    label, confidence = predict_image(image_input)
    transcribed = transcribe_microphone(audio_input)
    advice = smart_solution(label, transcribed)

    return f"ðŸ“¸ Predicted Disease: {label} ({confidence}%)\nðŸ—£ï¸ You said: {transcribed}\n\nðŸ’¡ Advice:\n{advice}"

# âœ… Launch Gradio app
gr.Interface(
    fn=process,
    inputs=[
        gr.Image(type="pil", label="Upload Leaf Image"),
        gr.Audio(type="filepath", label="Speak the problem", interactive=True)

    ],
    outputs="text",
    title="ðŸŒ¿ AI Farmer Helper",
    description="Upload a tomato leaf image and speak the issue. The app gives smart advice using AI."
).launch(debug=True)













































































